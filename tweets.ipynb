{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Classifying Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of a self-directed Codecademy certification, the purpose of this project is to use a Naive Bayes Classifier to find patterns in real tweets. \n",
    "\n",
    "The goal is to create a classification algorithm that can classify a tweet or sentence, and predict whether that sentence came from New York, London, or Paris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previewing the Data ##\n",
    "\n",
    "We will import and preview the dataset for each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Importing the datasets\n",
    "\n",
    "new_york_tweets = pd.read_json(\"new_york.json\", lines=True)\n",
    "london_tweets = pd.read_json(\"london.json\", lines=True)\n",
    "paris_tweets = pd.read_json(\"paris.json\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4723\n",
      "5341\n",
      "2510\n",
      "Index(['created_at', 'id', 'id_str', 'text', 'display_text_range', 'source',\n",
      "       'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
      "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
      "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
      "       'contributors', 'is_quote_status', 'quote_count', 'reply_count',\n",
      "       'retweet_count', 'favorite_count', 'entities', 'favorited', 'retweeted',\n",
      "       'filter_level', 'lang', 'timestamp_ms', 'extended_tweet',\n",
      "       'possibly_sensitive', 'quoted_status_id', 'quoted_status_id_str',\n",
      "       'quoted_status', 'quoted_status_permalink', 'extended_entities',\n",
      "       'withheld_in_countries'],\n",
      "      dtype='object')\n",
      "Index(['created_at', 'id', 'id_str', 'text', 'display_text_range', 'source',\n",
      "       'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
      "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
      "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
      "       'contributors', 'is_quote_status', 'extended_tweet', 'quote_count',\n",
      "       'reply_count', 'retweet_count', 'favorite_count', 'entities',\n",
      "       'favorited', 'retweeted', 'filter_level', 'lang', 'timestamp_ms',\n",
      "       'possibly_sensitive', 'quoted_status_id', 'quoted_status_id_str',\n",
      "       'quoted_status', 'quoted_status_permalink', 'extended_entities'],\n",
      "      dtype='object')\n",
      "Index(['created_at', 'id', 'id_str', 'text', 'source', 'truncated',\n",
      "       'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
      "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
      "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
      "       'contributors', 'is_quote_status', 'quote_count', 'reply_count',\n",
      "       'retweet_count', 'favorite_count', 'entities', 'favorited', 'retweeted',\n",
      "       'filter_level', 'lang', 'timestamp_ms', 'display_text_range',\n",
      "       'extended_entities', 'possibly_sensitive', 'quoted_status_id',\n",
      "       'quoted_status_id_str', 'quoted_status', 'quoted_status_permalink',\n",
      "       'extended_tweet'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#How many tweets do we have from each city?\n",
    "print(len(new_york_tweets))\n",
    "print(len(london_tweets))\n",
    "print(len(paris_tweets))\n",
    "\n",
    "#Which columns are present in each dataframe?\n",
    "print(new_york_tweets.columns)\n",
    "print(london_tweets.columns)\n",
    "print(paris_tweets.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier ##\n",
    "\n",
    "We will use a Naive Bayes Classifier to find patterns in the tweets that we have imported.\n",
    "\n",
    "We will use the integers 0, 1 and 2 to represent the cities; more specifically, **0** represents **New York**, **1** represents **London** and **2** represents **Paris**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_york_text = new_york_tweets[\"text\"].tolist()\n",
    "london_text = london_tweets[\"text\"].tolist()\n",
    "paris_text = paris_tweets[\"text\"].tolist()\n",
    "\n",
    "all_tweets = new_york_text + london_text + paris_text\n",
    "labels = [0] * len(new_york_text) + [1] * len(london_text) + [2] * len(paris_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Sets ##\n",
    "\n",
    "\n",
    "We will use scikit-learn's **train_test_split** function to split the dataset into a training set and testing set. We will train the model on 80% of the data, and use the other 20% to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The random_state parameter is set to 1, ensuring that the sets remain unchanged each time the code is run.\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_tweets, labels, test_size = 0.2, random_state=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectors ##\n",
    "\n",
    "We need to transform our tweets into count vectors for the Naive Bayes Classifier. \n",
    "\n",
    "For example, the count vector for the tweet \"I love chocolate and I love chocolate ice cream.\" would be\n",
    "\n",
    "{\"i\": 2, \"love\": 2, \"chocolate\": 2, \"and\": 1, \"ice\": 1, \"cream\": 1}\n",
    "\n",
    "We will do this using scikit-learn's **CountVectorizer** function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    " \n",
    "#We want to teach our counter the vocabulary from our training set\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "#Now we transform the training and testing data to count vectors\n",
    "train_counts = vectorizer.transform(X_train)\n",
    "\n",
    "test_counts = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing the Naive Bayes Classifier ##\n",
    "\n",
    "We will create a Naive Bayes Classifier using the scikit-learn package. \n",
    "\n",
    "We will fit the model to our training set, and using our testing set to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "#Fitting the model\n",
    "classifier.fit(train_counts, y_train)\n",
    "#Testing the model\n",
    "predictions = classifier.predict(test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Your Model ##\n",
    "\n",
    "We will evaluate the model using scikit-learn's **accuracy_score** function, which returns the percentage of tweets in the testing set that were correctly classified.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6779324055666004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model correctly predicted the location of around 68% (roughly 2/3) of the tweets in our testing set.\n",
    "\n",
    "We will also compute the confusion matrix for this model using scikit-learn's function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[541 404  28]\n",
      " [203 824  34]\n",
      " [ 38 103 340]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model correctly predicted 541 tweets originating from New York, 824 tweets originating from London, and 340 tweets originating from Paris. Interestingly, the model predicted that 203 tweets originated from New York when they actually originated from London. Similarly, the model predicted that 404 tweets originated from London when they actually originated from New York. In comparison, even accounting for the proportion of tweets from each city, the model rarely incorrectly classifies tweets originating from Paris. One reason for this may be that English is the main spoken language of London and New York, whereas French is the main spoken language of Paris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Model ##\n",
    "\n",
    "We will evaluate the model using scikit-learn's **accuracy_score** function, which returns the percentage of tweets in the testing set that were correctly classified.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3445515a86b35784e9ff0a3867460d917dede3b1e294f9e5a626e6161da088f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
